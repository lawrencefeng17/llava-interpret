{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f194c739-1099-472e-a56c-e533fae0ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the GPUs you want to use (e.g., GPUs 4, 5, 6, 7)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d178c84-980e-47ac-8d40-a45b4b5ee381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05700af430a34243805d1e132c3d06ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "import requests\n",
    "import torch\n",
    "import transformer_lens\n",
    "import transformers\n",
    "from tempfile import TemporaryDirectory\n",
    "from PIL import Image\n",
    "import circuitsvis as cv\n",
    "\n",
    "es = contextlib.ExitStack()\n",
    "es.enter_context(torch.inference_mode())\n",
    "\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = transformers.AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=0,\n",
    ")\n",
    "processor = transformers.AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f176543b-bedd-4479-bcf4-aa879b8bed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hooked_model(model, tokenizer):\n",
    "    with TemporaryDirectory() as model_name:\n",
    "        model.config.save_pretrained(model_name)\n",
    "        cfg = transformer_lens.loading.get_pretrained_model_config(\n",
    "            model_name,\n",
    "            device=1,\n",
    "            dtype=model.dtype,\n",
    "        )\n",
    "        state_dict = transformer_lens.loading.get_pretrained_state_dict(\n",
    "            model_name,\n",
    "            cfg,\n",
    "            model,\n",
    "        )\n",
    "    hooked_model = transformer_lens.HookedTransformer(cfg, tokenizer)\n",
    "    hooked_model.load_and_process_state_dict(state_dict)\n",
    "    return hooked_model\n",
    "\n",
    "def get_input_embeds(input_ids, pixel_values):\n",
    "    input_embeds = model.get_input_embeddings()(input_ids)\n",
    "    image_features = model.get_image_features(\n",
    "        pixel_values,\n",
    "        model.config.vision_feature_layer,\n",
    "        model.config.vision_feature_select_strategy,\n",
    "    )\n",
    "    # Ensure input_ids is on the same device as input_embeds\n",
    "    input_ids = input_ids.to(input_embeds.device)\n",
    "    # Replace image_token_index (=32000) with image_feature tokens\n",
    "    input_embeds[input_ids == model.config.image_token_index] = image_features\n",
    "    return input_embeds\n",
    "\n",
    "def run_model(prompt, image):\n",
    "    inp = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_embeds = get_input_embeds(inp.input_ids, inp.pixel_values)\n",
    "    return hooked_model.run_with_cache(input_embeds, start_at_layer=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1972a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify text generation\n",
    "conv = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"describe the image\"},\n",
    "        ]},\n",
    "    ]\n",
    "prompt = processor.apply_chat_template(conv, add_generation_prompt=True)\n",
    "\n",
    "image = requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True)\n",
    "image = Image.open(image.raw)\n",
    "inp = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model(\n",
    "    **inp,\n",
    "    output_hidden_states=True,\n",
    "    num_logits_to_keep=1,\n",
    "    use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f934fb2a-f491-4513-bc11-1b2834db8dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    }
   ],
   "source": [
    "hooked_model = get_hooked_model(model.language_model, processor.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d84ab28e-a33c-40df-93d0-da58bc2aab40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conversation = [{\"role\": \"user\", \"content\": [\n",
    "    {\"type\": \"image\"},\n",
    "    {\"type\": \"text\", \"text\": \"describe the image\"},\n",
    "]}]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "image = requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True)\n",
    "image = Image.open(image.raw)\n",
    "\n",
    "logits, activations = run_model(prompt, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef0e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a517a-5d6e-4202-ac5a-5c5979c7f83c",
   "metadata": {},
   "source": [
    "# Test hooked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc962e29-9a14-4adf-851c-6f6dc95e9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "def test_llava_setup(\n",
    "    model, \n",
    "    processor, \n",
    "    hooked_model, \n",
    "    test_image_url: str = \"https://raw.githubusercontent.com/llava-forge/llava-2/main/images/llava2_logo.png\",\n",
    "    test_prompt: str = \"What do you see in this image?\"\n",
    ") -> Tuple[bool, Dict]:\n",
    "    \"\"\"\n",
    "    Comprehensive test suite for LLaVA + TransformerLens setup.\n",
    "    Returns (success_flag, diagnostics_dict)\n",
    "    \"\"\"\n",
    "    diagnostics = {}\n",
    "\n",
    "    conversation = [{\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": test_prompt},\n",
    "    ]}]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    \n",
    "    # 1. Test image processing\n",
    "    try:\n",
    "        image = Image.open(requests.get(test_image_url, stream=True).raw)\n",
    "        inp = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "        diagnostics['image_processing'] = {\n",
    "            'success': True,\n",
    "            'input_shape': inp.pixel_values.shape,\n",
    "            'device': inp.pixel_values.device\n",
    "        }\n",
    "    except Exception as e:\n",
    "        diagnostics['image_processing'] = {'success': False, 'error': str(e)}\n",
    "        return False, diagnostics\n",
    "\n",
    "    # 2. Test embedding generation\n",
    "    try:\n",
    "        input_embeds = get_input_embeds(inp.input_ids, inp.pixel_values)\n",
    "        diagnostics['embeddings'] = {\n",
    "            'success': True,\n",
    "            'shape': input_embeds.shape,\n",
    "            'dtype': input_embeds.dtype,\n",
    "            'device': input_embeds.device,\n",
    "            'non_zero': torch.any(input_embeds != 0).item(),\n",
    "            'mean': input_embeds.mean().item(),\n",
    "            'std': input_embeds.std().item()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        diagnostics['embeddings'] = {'success': False, 'error': str(e)}\n",
    "        return False, diagnostics\n",
    "\n",
    "    # 3. Test hooked model forward pass\n",
    "    try:\n",
    "        logits, cache = hooked_model.run_with_cache(\n",
    "            input_embeds,\n",
    "            start_at_layer=0,\n",
    "            return_type='logits'\n",
    "        )\n",
    "        diagnostics['hooked_forward'] = {\n",
    "            'success': True,\n",
    "            'logits_shape': logits.shape,\n",
    "            'cache_keys': list(cache.keys()),\n",
    "            'num_layers': len([k for k in cache.keys() if 'pattern' in k])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        diagnostics['hooked_forward'] = {'success': False, 'error': str(e)}\n",
    "        return False, diagnostics\n",
    "\n",
    "    # 4. Basic sanity checks\n",
    "    checks = {\n",
    "        'embedding_dim_match': input_embeds.shape[-1] == hooked_model.cfg.d_model,\n",
    "        'layer_count_match': len([k for k in cache.keys() if 'pattern' in k]) == hooked_model.cfg.n_layers,\n",
    "        'output_vocab_size': logits.shape[-1] == hooked_model.cfg.d_vocab\n",
    "    }\n",
    "    diagnostics['sanity_checks'] = checks\n",
    "\n",
    "    success = all(diagnostics['sanity_checks'].values())\n",
    "    \n",
    "    return success, diagnostics\n",
    "\n",
    "def visualize_attention(cache, layer: int = 0, head: int = 0):\n",
    "    \"\"\"\n",
    "    Visualize attention patterns for a specific layer and head\n",
    "    \"\"\"\n",
    "    attention_pattern = cache[f'pattern.{layer}.{head}'][0]  # Get first batch item\n",
    "    return cv.attention.attention_patterns(\n",
    "        tokens=hooked_model.to_str_tokens(attention_pattern),\n",
    "        attention=attention_pattern\n",
    "    )\n",
    "\n",
    "def run_verification_test(image_url: str, prompt: str):\n",
    "    \"\"\"\n",
    "    Run the full verification suite and print results\n",
    "    \"\"\"\n",
    "    success, diagnostics = test_llava_setup(model, processor, hooked_model, image_url, prompt)\n",
    "    \n",
    "    print(f\"🔍 LLaVA + TransformerLens Verification Results:\")\n",
    "    print(f\"Overall Success: {'✅' if success else '❌'}\\n\")\n",
    "    \n",
    "    for stage, results in diagnostics.items():\n",
    "        print(f\"\\n{stage.upper()}:\")\n",
    "        if isinstance(results, dict):\n",
    "            for key, value in results.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"  {results}\")\n",
    "            \n",
    "    return success, diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7bc113-9d09-4584-a42f-d88df4b24ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 LLaVA + TransformerLens Verification Results:\n",
      "Overall Success: ✅\n",
      "\n",
      "\n",
      "IMAGE_PROCESSING:\n",
      "  success: True\n",
      "  input_shape: torch.Size([1, 3, 336, 336])\n",
      "  device: cuda:0\n",
      "\n",
      "EMBEDDINGS:\n",
      "  success: True\n",
      "  shape: torch.Size([1, 596, 4096])\n",
      "  dtype: torch.float16\n",
      "  device: cuda:0\n",
      "  non_zero: True\n",
      "  mean: -0.00444793701171875\n",
      "  std: 0.9052734375\n",
      "\n",
      "HOOKED_FORWARD:\n",
      "  success: True\n",
      "  logits_shape: torch.Size([1, 596, 32064])\n",
      "  cache_keys: ['blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_rot_q', 'blocks.0.attn.hook_rot_k', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_pre_linear', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_rot_q', 'blocks.1.attn.hook_rot_k', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_pre_linear', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_rot_q', 'blocks.2.attn.hook_rot_k', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_pre_linear', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_rot_q', 'blocks.3.attn.hook_rot_k', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_pre_linear', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_rot_q', 'blocks.4.attn.hook_rot_k', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_pre_linear', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_rot_q', 'blocks.5.attn.hook_rot_k', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_pre_linear', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_rot_q', 'blocks.6.attn.hook_rot_k', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_pre_linear', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_rot_q', 'blocks.7.attn.hook_rot_k', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_pre_linear', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_rot_q', 'blocks.8.attn.hook_rot_k', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_pre_linear', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_rot_q', 'blocks.9.attn.hook_rot_k', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_pre_linear', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_rot_q', 'blocks.10.attn.hook_rot_k', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_pre_linear', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_rot_q', 'blocks.11.attn.hook_rot_k', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_pre_linear', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'blocks.12.hook_resid_pre', 'blocks.12.ln1.hook_scale', 'blocks.12.ln1.hook_normalized', 'blocks.12.attn.hook_q', 'blocks.12.attn.hook_k', 'blocks.12.attn.hook_v', 'blocks.12.attn.hook_rot_q', 'blocks.12.attn.hook_rot_k', 'blocks.12.attn.hook_attn_scores', 'blocks.12.attn.hook_pattern', 'blocks.12.attn.hook_z', 'blocks.12.hook_attn_out', 'blocks.12.hook_resid_mid', 'blocks.12.ln2.hook_scale', 'blocks.12.ln2.hook_normalized', 'blocks.12.mlp.hook_pre', 'blocks.12.mlp.hook_pre_linear', 'blocks.12.mlp.hook_post', 'blocks.12.hook_mlp_out', 'blocks.12.hook_resid_post', 'blocks.13.hook_resid_pre', 'blocks.13.ln1.hook_scale', 'blocks.13.ln1.hook_normalized', 'blocks.13.attn.hook_q', 'blocks.13.attn.hook_k', 'blocks.13.attn.hook_v', 'blocks.13.attn.hook_rot_q', 'blocks.13.attn.hook_rot_k', 'blocks.13.attn.hook_attn_scores', 'blocks.13.attn.hook_pattern', 'blocks.13.attn.hook_z', 'blocks.13.hook_attn_out', 'blocks.13.hook_resid_mid', 'blocks.13.ln2.hook_scale', 'blocks.13.ln2.hook_normalized', 'blocks.13.mlp.hook_pre', 'blocks.13.mlp.hook_pre_linear', 'blocks.13.mlp.hook_post', 'blocks.13.hook_mlp_out', 'blocks.13.hook_resid_post', 'blocks.14.hook_resid_pre', 'blocks.14.ln1.hook_scale', 'blocks.14.ln1.hook_normalized', 'blocks.14.attn.hook_q', 'blocks.14.attn.hook_k', 'blocks.14.attn.hook_v', 'blocks.14.attn.hook_rot_q', 'blocks.14.attn.hook_rot_k', 'blocks.14.attn.hook_attn_scores', 'blocks.14.attn.hook_pattern', 'blocks.14.attn.hook_z', 'blocks.14.hook_attn_out', 'blocks.14.hook_resid_mid', 'blocks.14.ln2.hook_scale', 'blocks.14.ln2.hook_normalized', 'blocks.14.mlp.hook_pre', 'blocks.14.mlp.hook_pre_linear', 'blocks.14.mlp.hook_post', 'blocks.14.hook_mlp_out', 'blocks.14.hook_resid_post', 'blocks.15.hook_resid_pre', 'blocks.15.ln1.hook_scale', 'blocks.15.ln1.hook_normalized', 'blocks.15.attn.hook_q', 'blocks.15.attn.hook_k', 'blocks.15.attn.hook_v', 'blocks.15.attn.hook_rot_q', 'blocks.15.attn.hook_rot_k', 'blocks.15.attn.hook_attn_scores', 'blocks.15.attn.hook_pattern', 'blocks.15.attn.hook_z', 'blocks.15.hook_attn_out', 'blocks.15.hook_resid_mid', 'blocks.15.ln2.hook_scale', 'blocks.15.ln2.hook_normalized', 'blocks.15.mlp.hook_pre', 'blocks.15.mlp.hook_pre_linear', 'blocks.15.mlp.hook_post', 'blocks.15.hook_mlp_out', 'blocks.15.hook_resid_post', 'blocks.16.hook_resid_pre', 'blocks.16.ln1.hook_scale', 'blocks.16.ln1.hook_normalized', 'blocks.16.attn.hook_q', 'blocks.16.attn.hook_k', 'blocks.16.attn.hook_v', 'blocks.16.attn.hook_rot_q', 'blocks.16.attn.hook_rot_k', 'blocks.16.attn.hook_attn_scores', 'blocks.16.attn.hook_pattern', 'blocks.16.attn.hook_z', 'blocks.16.hook_attn_out', 'blocks.16.hook_resid_mid', 'blocks.16.ln2.hook_scale', 'blocks.16.ln2.hook_normalized', 'blocks.16.mlp.hook_pre', 'blocks.16.mlp.hook_pre_linear', 'blocks.16.mlp.hook_post', 'blocks.16.hook_mlp_out', 'blocks.16.hook_resid_post', 'blocks.17.hook_resid_pre', 'blocks.17.ln1.hook_scale', 'blocks.17.ln1.hook_normalized', 'blocks.17.attn.hook_q', 'blocks.17.attn.hook_k', 'blocks.17.attn.hook_v', 'blocks.17.attn.hook_rot_q', 'blocks.17.attn.hook_rot_k', 'blocks.17.attn.hook_attn_scores', 'blocks.17.attn.hook_pattern', 'blocks.17.attn.hook_z', 'blocks.17.hook_attn_out', 'blocks.17.hook_resid_mid', 'blocks.17.ln2.hook_scale', 'blocks.17.ln2.hook_normalized', 'blocks.17.mlp.hook_pre', 'blocks.17.mlp.hook_pre_linear', 'blocks.17.mlp.hook_post', 'blocks.17.hook_mlp_out', 'blocks.17.hook_resid_post', 'blocks.18.hook_resid_pre', 'blocks.18.ln1.hook_scale', 'blocks.18.ln1.hook_normalized', 'blocks.18.attn.hook_q', 'blocks.18.attn.hook_k', 'blocks.18.attn.hook_v', 'blocks.18.attn.hook_rot_q', 'blocks.18.attn.hook_rot_k', 'blocks.18.attn.hook_attn_scores', 'blocks.18.attn.hook_pattern', 'blocks.18.attn.hook_z', 'blocks.18.hook_attn_out', 'blocks.18.hook_resid_mid', 'blocks.18.ln2.hook_scale', 'blocks.18.ln2.hook_normalized', 'blocks.18.mlp.hook_pre', 'blocks.18.mlp.hook_pre_linear', 'blocks.18.mlp.hook_post', 'blocks.18.hook_mlp_out', 'blocks.18.hook_resid_post', 'blocks.19.hook_resid_pre', 'blocks.19.ln1.hook_scale', 'blocks.19.ln1.hook_normalized', 'blocks.19.attn.hook_q', 'blocks.19.attn.hook_k', 'blocks.19.attn.hook_v', 'blocks.19.attn.hook_rot_q', 'blocks.19.attn.hook_rot_k', 'blocks.19.attn.hook_attn_scores', 'blocks.19.attn.hook_pattern', 'blocks.19.attn.hook_z', 'blocks.19.hook_attn_out', 'blocks.19.hook_resid_mid', 'blocks.19.ln2.hook_scale', 'blocks.19.ln2.hook_normalized', 'blocks.19.mlp.hook_pre', 'blocks.19.mlp.hook_pre_linear', 'blocks.19.mlp.hook_post', 'blocks.19.hook_mlp_out', 'blocks.19.hook_resid_post', 'blocks.20.hook_resid_pre', 'blocks.20.ln1.hook_scale', 'blocks.20.ln1.hook_normalized', 'blocks.20.attn.hook_q', 'blocks.20.attn.hook_k', 'blocks.20.attn.hook_v', 'blocks.20.attn.hook_rot_q', 'blocks.20.attn.hook_rot_k', 'blocks.20.attn.hook_attn_scores', 'blocks.20.attn.hook_pattern', 'blocks.20.attn.hook_z', 'blocks.20.hook_attn_out', 'blocks.20.hook_resid_mid', 'blocks.20.ln2.hook_scale', 'blocks.20.ln2.hook_normalized', 'blocks.20.mlp.hook_pre', 'blocks.20.mlp.hook_pre_linear', 'blocks.20.mlp.hook_post', 'blocks.20.hook_mlp_out', 'blocks.20.hook_resid_post', 'blocks.21.hook_resid_pre', 'blocks.21.ln1.hook_scale', 'blocks.21.ln1.hook_normalized', 'blocks.21.attn.hook_q', 'blocks.21.attn.hook_k', 'blocks.21.attn.hook_v', 'blocks.21.attn.hook_rot_q', 'blocks.21.attn.hook_rot_k', 'blocks.21.attn.hook_attn_scores', 'blocks.21.attn.hook_pattern', 'blocks.21.attn.hook_z', 'blocks.21.hook_attn_out', 'blocks.21.hook_resid_mid', 'blocks.21.ln2.hook_scale', 'blocks.21.ln2.hook_normalized', 'blocks.21.mlp.hook_pre', 'blocks.21.mlp.hook_pre_linear', 'blocks.21.mlp.hook_post', 'blocks.21.hook_mlp_out', 'blocks.21.hook_resid_post', 'blocks.22.hook_resid_pre', 'blocks.22.ln1.hook_scale', 'blocks.22.ln1.hook_normalized', 'blocks.22.attn.hook_q', 'blocks.22.attn.hook_k', 'blocks.22.attn.hook_v', 'blocks.22.attn.hook_rot_q', 'blocks.22.attn.hook_rot_k', 'blocks.22.attn.hook_attn_scores', 'blocks.22.attn.hook_pattern', 'blocks.22.attn.hook_z', 'blocks.22.hook_attn_out', 'blocks.22.hook_resid_mid', 'blocks.22.ln2.hook_scale', 'blocks.22.ln2.hook_normalized', 'blocks.22.mlp.hook_pre', 'blocks.22.mlp.hook_pre_linear', 'blocks.22.mlp.hook_post', 'blocks.22.hook_mlp_out', 'blocks.22.hook_resid_post', 'blocks.23.hook_resid_pre', 'blocks.23.ln1.hook_scale', 'blocks.23.ln1.hook_normalized', 'blocks.23.attn.hook_q', 'blocks.23.attn.hook_k', 'blocks.23.attn.hook_v', 'blocks.23.attn.hook_rot_q', 'blocks.23.attn.hook_rot_k', 'blocks.23.attn.hook_attn_scores', 'blocks.23.attn.hook_pattern', 'blocks.23.attn.hook_z', 'blocks.23.hook_attn_out', 'blocks.23.hook_resid_mid', 'blocks.23.ln2.hook_scale', 'blocks.23.ln2.hook_normalized', 'blocks.23.mlp.hook_pre', 'blocks.23.mlp.hook_pre_linear', 'blocks.23.mlp.hook_post', 'blocks.23.hook_mlp_out', 'blocks.23.hook_resid_post', 'blocks.24.hook_resid_pre', 'blocks.24.ln1.hook_scale', 'blocks.24.ln1.hook_normalized', 'blocks.24.attn.hook_q', 'blocks.24.attn.hook_k', 'blocks.24.attn.hook_v', 'blocks.24.attn.hook_rot_q', 'blocks.24.attn.hook_rot_k', 'blocks.24.attn.hook_attn_scores', 'blocks.24.attn.hook_pattern', 'blocks.24.attn.hook_z', 'blocks.24.hook_attn_out', 'blocks.24.hook_resid_mid', 'blocks.24.ln2.hook_scale', 'blocks.24.ln2.hook_normalized', 'blocks.24.mlp.hook_pre', 'blocks.24.mlp.hook_pre_linear', 'blocks.24.mlp.hook_post', 'blocks.24.hook_mlp_out', 'blocks.24.hook_resid_post', 'blocks.25.hook_resid_pre', 'blocks.25.ln1.hook_scale', 'blocks.25.ln1.hook_normalized', 'blocks.25.attn.hook_q', 'blocks.25.attn.hook_k', 'blocks.25.attn.hook_v', 'blocks.25.attn.hook_rot_q', 'blocks.25.attn.hook_rot_k', 'blocks.25.attn.hook_attn_scores', 'blocks.25.attn.hook_pattern', 'blocks.25.attn.hook_z', 'blocks.25.hook_attn_out', 'blocks.25.hook_resid_mid', 'blocks.25.ln2.hook_scale', 'blocks.25.ln2.hook_normalized', 'blocks.25.mlp.hook_pre', 'blocks.25.mlp.hook_pre_linear', 'blocks.25.mlp.hook_post', 'blocks.25.hook_mlp_out', 'blocks.25.hook_resid_post', 'blocks.26.hook_resid_pre', 'blocks.26.ln1.hook_scale', 'blocks.26.ln1.hook_normalized', 'blocks.26.attn.hook_q', 'blocks.26.attn.hook_k', 'blocks.26.attn.hook_v', 'blocks.26.attn.hook_rot_q', 'blocks.26.attn.hook_rot_k', 'blocks.26.attn.hook_attn_scores', 'blocks.26.attn.hook_pattern', 'blocks.26.attn.hook_z', 'blocks.26.hook_attn_out', 'blocks.26.hook_resid_mid', 'blocks.26.ln2.hook_scale', 'blocks.26.ln2.hook_normalized', 'blocks.26.mlp.hook_pre', 'blocks.26.mlp.hook_pre_linear', 'blocks.26.mlp.hook_post', 'blocks.26.hook_mlp_out', 'blocks.26.hook_resid_post', 'blocks.27.hook_resid_pre', 'blocks.27.ln1.hook_scale', 'blocks.27.ln1.hook_normalized', 'blocks.27.attn.hook_q', 'blocks.27.attn.hook_k', 'blocks.27.attn.hook_v', 'blocks.27.attn.hook_rot_q', 'blocks.27.attn.hook_rot_k', 'blocks.27.attn.hook_attn_scores', 'blocks.27.attn.hook_pattern', 'blocks.27.attn.hook_z', 'blocks.27.hook_attn_out', 'blocks.27.hook_resid_mid', 'blocks.27.ln2.hook_scale', 'blocks.27.ln2.hook_normalized', 'blocks.27.mlp.hook_pre', 'blocks.27.mlp.hook_pre_linear', 'blocks.27.mlp.hook_post', 'blocks.27.hook_mlp_out', 'blocks.27.hook_resid_post', 'blocks.28.hook_resid_pre', 'blocks.28.ln1.hook_scale', 'blocks.28.ln1.hook_normalized', 'blocks.28.attn.hook_q', 'blocks.28.attn.hook_k', 'blocks.28.attn.hook_v', 'blocks.28.attn.hook_rot_q', 'blocks.28.attn.hook_rot_k', 'blocks.28.attn.hook_attn_scores', 'blocks.28.attn.hook_pattern', 'blocks.28.attn.hook_z', 'blocks.28.hook_attn_out', 'blocks.28.hook_resid_mid', 'blocks.28.ln2.hook_scale', 'blocks.28.ln2.hook_normalized', 'blocks.28.mlp.hook_pre', 'blocks.28.mlp.hook_pre_linear', 'blocks.28.mlp.hook_post', 'blocks.28.hook_mlp_out', 'blocks.28.hook_resid_post', 'blocks.29.hook_resid_pre', 'blocks.29.ln1.hook_scale', 'blocks.29.ln1.hook_normalized', 'blocks.29.attn.hook_q', 'blocks.29.attn.hook_k', 'blocks.29.attn.hook_v', 'blocks.29.attn.hook_rot_q', 'blocks.29.attn.hook_rot_k', 'blocks.29.attn.hook_attn_scores', 'blocks.29.attn.hook_pattern', 'blocks.29.attn.hook_z', 'blocks.29.hook_attn_out', 'blocks.29.hook_resid_mid', 'blocks.29.ln2.hook_scale', 'blocks.29.ln2.hook_normalized', 'blocks.29.mlp.hook_pre', 'blocks.29.mlp.hook_pre_linear', 'blocks.29.mlp.hook_post', 'blocks.29.hook_mlp_out', 'blocks.29.hook_resid_post', 'blocks.30.hook_resid_pre', 'blocks.30.ln1.hook_scale', 'blocks.30.ln1.hook_normalized', 'blocks.30.attn.hook_q', 'blocks.30.attn.hook_k', 'blocks.30.attn.hook_v', 'blocks.30.attn.hook_rot_q', 'blocks.30.attn.hook_rot_k', 'blocks.30.attn.hook_attn_scores', 'blocks.30.attn.hook_pattern', 'blocks.30.attn.hook_z', 'blocks.30.hook_attn_out', 'blocks.30.hook_resid_mid', 'blocks.30.ln2.hook_scale', 'blocks.30.ln2.hook_normalized', 'blocks.30.mlp.hook_pre', 'blocks.30.mlp.hook_pre_linear', 'blocks.30.mlp.hook_post', 'blocks.30.hook_mlp_out', 'blocks.30.hook_resid_post', 'blocks.31.hook_resid_pre', 'blocks.31.ln1.hook_scale', 'blocks.31.ln1.hook_normalized', 'blocks.31.attn.hook_q', 'blocks.31.attn.hook_k', 'blocks.31.attn.hook_v', 'blocks.31.attn.hook_rot_q', 'blocks.31.attn.hook_rot_k', 'blocks.31.attn.hook_attn_scores', 'blocks.31.attn.hook_pattern', 'blocks.31.attn.hook_z', 'blocks.31.hook_attn_out', 'blocks.31.hook_resid_mid', 'blocks.31.ln2.hook_scale', 'blocks.31.ln2.hook_normalized', 'blocks.31.mlp.hook_pre', 'blocks.31.mlp.hook_pre_linear', 'blocks.31.mlp.hook_post', 'blocks.31.hook_mlp_out', 'blocks.31.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']\n",
      "  num_layers: 32\n",
      "\n",
      "SANITY_CHECKS:\n",
      "  embedding_dim_match: True\n",
      "  layer_count_match: True\n",
      "  output_vocab_size: True\n"
     ]
    }
   ],
   "source": [
    "success, diagnostics = run_verification_test(\n",
    "    \"https://llava-vl.github.io/static/images/view.jpg\",\n",
    "    \"What do you see in this image?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752bbd83",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f58f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f6f8497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "def analyze_attention_patterns(\n",
    "    cache,\n",
    "    hooked_model,\n",
    "    input_ids,\n",
    "    num_image_tokens: int,\n",
    "    layer: Optional[int] = None,\n",
    "    head: Optional[int] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze attention patterns between text and image tokens.\n",
    "    Adapted for LLaVA's cache structure.\n",
    "    \"\"\"\n",
    "    # Get token strings for visualization\n",
    "    tokens = hooked_model.to_str_tokens(input_ids[0])\n",
    "    \n",
    "    # Find image token positions (usually at the start for LLaVA)\n",
    "    image_token_positions = torch.where(input_ids[0] == model.config.image_token_index)[0]\n",
    "    text_positions = [i for i in range(len(tokens)) if i not in image_token_positions]\n",
    "    \n",
    "    layers = [layer] if layer is not None else range(hooked_model.cfg.n_layers)\n",
    "    heads = [head] if head is not None else range(hooked_model.cfg.n_heads)\n",
    "    \n",
    "    attention_stats = {\n",
    "        'text_to_image': torch.zeros(len(layers), len(heads)).to(cache['blocks.0.attn.hook_pattern'].device),\n",
    "        'image_to_text': torch.zeros(len(layers), len(heads)).to(cache['blocks.0.attn.hook_pattern'].device),\n",
    "        'self_attention': torch.zeros(len(layers), len(heads)).to(cache['blocks.0.attn.hook_pattern'].device)\n",
    "    }\n",
    "    \n",
    "    for l in layers:\n",
    "        for h in heads:\n",
    "            # Access pattern using LLaVA's cache structure\n",
    "            pattern = cache[f'blocks.{l}.attn.hook_pattern'][0]  # [seq_len, seq_len]\n",
    "            \n",
    "            # Calculate average attention scores\n",
    "            # Move indicies to same device as pattern\n",
    "            text_positions = torch.tensor(text_positions).to(pattern.device)\n",
    "            image_token_positions = torch.tensor(image_token_positions).to(pattern.device)\n",
    "            text_to_image = pattern[text_positions][:, image_token_positions].mean()\n",
    "            image_to_text = pattern[image_token_positions][:, text_positions].mean()\n",
    "            self_attention = pattern.diagonal().mean()\n",
    "            \n",
    "            attention_stats['text_to_image'][l, h] = text_to_image\n",
    "            attention_stats['image_to_text'][l, h] = image_to_text\n",
    "            attention_stats['self_attention'][l, h] = self_attention\n",
    "    \n",
    "    return attention_stats\n",
    "\n",
    "def run_attention_analysis(prompt: str, image, model, processor, hooked_model):\n",
    "    \"\"\"\n",
    "    Run complete attention analysis pipeline with LLaVA's conversation template\n",
    "    \"\"\"\n",
    "    # Apply LLaVA's conversation template\n",
    "    conversation = [{\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": prompt},\n",
    "    ]}]\n",
    "    formatted_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    \n",
    "    # Process input\n",
    "    inp = processor(image, formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_embeds = get_input_embeds(inp.input_ids, inp.pixel_values)\n",
    "    \n",
    "    # Run model with cache\n",
    "    _, cache = hooked_model.run_with_cache(\n",
    "        input_embeds,\n",
    "        start_at_layer=0,\n",
    "        return_type='logits'\n",
    "    )\n",
    "    \n",
    "    # Count image tokens\n",
    "    num_image_tokens = (inp.input_ids[0] == model.config.image_token_index).sum().item()\n",
    "    \n",
    "    # Analyze attention patterns\n",
    "    attention_stats = analyze_attention_patterns(\n",
    "        cache,\n",
    "        hooked_model,\n",
    "        inp.input_ids,\n",
    "        num_image_tokens\n",
    "    )\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    metrics = ['text_to_image', 'image_to_text', 'self_attention']\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        sns.heatmap(\n",
    "            attention_stats[metric],\n",
    "            ax=axes[idx],\n",
    "            cmap='viridis',\n",
    "            xticklabels=[f'H{i}' for i in range(attention_stats[metric].shape[1])],\n",
    "            yticklabels=[f'L{i}' for i in range(attention_stats[metric].shape[0])]\n",
    "        )\n",
    "        axes[idx].set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "        axes[idx].set_xlabel('Heads')\n",
    "        axes[idx].set_ylabel('Layers')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cache, attention_stats\n",
    "\n",
    "def visualize_layer_attention(\n",
    "    cache,\n",
    "    hooked_model,\n",
    "    input_ids,\n",
    "    layer: int,\n",
    "    head: int,\n",
    "    tokens: Optional[List[str]] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize attention patterns for a specific layer and head\n",
    "    \"\"\"\n",
    "    if tokens is None:\n",
    "        tokens = hooked_model.to_str_tokens(input_ids[0])\n",
    "    \n",
    "    pattern = cache[f'blocks.{layer}.attn.hook_pattern'][0]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        pattern.cpu(),\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.title(f'Attention Pattern (Layer {layer}, Head {head})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df6f2ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/anaconda3/envs/interp/lib/python3.12/site-packages/torch/__init__.py:1021: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n",
      "/tmp/ipykernel_3730307/3080627566.py:8: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "\n",
    "# Delete all variables that are using CUDA memory\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            del obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Run garbage collector\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a35d059d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3730307/599027433.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image_token_positions = torch.tensor(image_token_positions).to(pattern.device)\n",
      "/tmp/ipykernel_3730307/599027433.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  text_positions = torch.tensor(text_positions).to(pattern.device)\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [3630,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Run the analysis\u001b[39;00m\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat do you see in this image?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m cache, attention_stats \u001b[38;5;241m=\u001b[39m \u001b[43mrun_attention_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooked_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 80\u001b[0m, in \u001b[0;36mrun_attention_analysis\u001b[0;34m(prompt, image, model, processor, hooked_model)\u001b[0m\n\u001b[1;32m     77\u001b[0m num_image_tokens \u001b[38;5;241m=\u001b[39m (inp\u001b[38;5;241m.\u001b[39minput_ids[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mimage_token_index)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Analyze attention patterns\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m attention_stats \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_attention_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooked_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_image_tokens\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Visualizations\u001b[39;00m\n\u001b[1;32m     88\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[0;32mIn[23], line 44\u001b[0m, in \u001b[0;36manalyze_attention_patterns\u001b[0;34m(cache, hooked_model, input_ids, num_image_tokens, layer, head)\u001b[0m\n\u001b[1;32m     42\u001b[0m text_positions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(text_positions)\u001b[38;5;241m.\u001b[39mto(pattern\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     43\u001b[0m image_token_positions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(image_token_positions)\u001b[38;5;241m.\u001b[39mto(pattern\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 44\u001b[0m text_to_image \u001b[38;5;241m=\u001b[39m \u001b[43mpattern\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_positions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_token_positions\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     45\u001b[0m image_to_text \u001b[38;5;241m=\u001b[39m pattern[image_token_positions][:, text_positions]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     46\u001b[0m self_attention \u001b[38;5;241m=\u001b[39m pattern\u001b[38;5;241m.\u001b[39mdiagonal()\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "image_url = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "# Run the analysis\n",
    "prompt = \"What do you see in this image?\"\n",
    "cache, attention_stats = run_attention_analysis(prompt, image, model, processor, hooked_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ef6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
