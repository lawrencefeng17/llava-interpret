{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72734576",
   "metadata": {},
   "source": [
    "# Transformer Analysis\n",
    " Using the TranformerLens and SAELens libraries we analyzed the behavior of LLaVA (fine tuned Gemma-2B) using a pre-trained SAE on Gemma-2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bb1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import requests\n",
    "import torch\n",
    "import transformer_lens\n",
    "import transformers\n",
    "from tempfile import TemporaryDirectory\n",
    "from PIL import Image\n",
    "\n",
    "es = contextlib.ExitStack()\n",
    "es.enter_context(torch.inference_mode())\n",
    "\n",
    "model_name = \"Intel/llava-gemma-2b\"\n",
    "model = transformers.AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = transformers.AutoProcessor.from_pretrained(model_name)\n",
    "processor.patch_size = model.config.vision_config.patch_size\n",
    "processor.vision_feature_select_strategy = model.config.vision_feature_select_strategy\n",
    "\n",
    "def get_hooked_model(model, tokenizer):\n",
    "    with TemporaryDirectory() as model_name:\n",
    "        model.config.save_pretrained(model_name)\n",
    "        cfg = transformer_lens.loading.get_pretrained_model_config(\n",
    "            model_name,\n",
    "            device=model.device,\n",
    "            dtype=model.dtype,\n",
    "        )\n",
    "        state_dict = transformer_lens.loading.get_pretrained_state_dict(\n",
    "            model_name,\n",
    "            cfg,\n",
    "            model,\n",
    "        )\n",
    "        for k, v in state_dict.items():\n",
    "            if v.device != model.device:\n",
    "                state_dict[k] = v.to(model.device)\n",
    "    hooked_model = transformer_lens.HookedTransformer(cfg, tokenizer)\n",
    "    hooked_model.load_and_process_state_dict(\n",
    "        state_dict,\n",
    "        fold_ln=False,\n",
    "        center_writing_weights=False,\n",
    "        center_unembed=False,\n",
    "        fold_value_biases=False,\n",
    "    )\n",
    "    return hooked_model\n",
    "\n",
    "def get_input_embeds(input_ids, pixel_values):\n",
    "    input_embeds = model.get_input_embeddings()(input_ids)\n",
    "    image_features = model.get_image_features(\n",
    "        pixel_values,\n",
    "        model.config.vision_feature_layer,\n",
    "        model.config.vision_feature_select_strategy,\n",
    "    )\n",
    "    input_embeds[input_ids == model.config.image_token_index] = image_features\n",
    "    return input_embeds\n",
    "\n",
    "def embed(prompt, image):\n",
    "    inp = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_embeds = get_input_embeds(inp.input_ids, inp.pixel_values)\n",
    "    input_embeds *= hooked_model.cfg.d_model ** 0.5\n",
    "    return input_embeds\n",
    "\n",
    "\n",
    "def run_model(prompt, image, **kwds):\n",
    "    input_embeds = embed(prompt, image)\n",
    "    return hooked_model.run_with_cache(input_embeds, start_at_layer=0, **kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b304a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_model = get_hooked_model(model.language_model, processor.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "287f6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [{\"role\": \"user\", \"content\": processor.image_token + \"\\ndescribe the image\"}]\n",
    "prompt = processor.tokenizer.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "image = requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True)\n",
    "image = Image.open(image.raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86471c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "import transformer_lens.utils as utils\n",
    "\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience.\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gemma-2b-res-jb\",  # <- Release name\n",
    "    sae_id=\"blocks.12.hook_resid_post\",  # <- SAE id (not always a hook point!)\n",
    "    device='cuda:0',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6da7812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hook(idx, amt):\n",
    "    encoded = torch.zeros(sae.cfg.d_sae, device=model.device)\n",
    "    bias = sae.decode(encoded)\n",
    "    encoded[idx] = amt\n",
    "    decoded = sae.decode(encoded) - bias\n",
    "    def boost(value, hook):\n",
    "        value += decoded\n",
    "        return value\n",
    "    return [(sae.cfg.hook_name, boost)]\n",
    "\n",
    "def hooked_generate(prompt, image, **kwds):\n",
    "    max_tokens = 50\n",
    "    input_embeds = embed(prompt, image)\n",
    "    token = 0\n",
    "    while token < max_tokens:\n",
    "        next_token = hooked_model.run_with_hooks(input_embeds, start_at_layer=0, **kwds)[0, -1].argmax()\n",
    "        if next_token.item() in {processor.tokenizer.eos_token_id, 107}:\n",
    "            print(\"EOS\")\n",
    "            break\n",
    "        print(end=processor.tokenizer.decode(next_token), flush=True)\n",
    "        token += 1\n",
    "        input_embeds = torch.cat([input_embeds, hooked_model.embed(next_token).unsqueeze(0).unsqueeze(0)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72d71b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = sae.encode(run_model(prompt, image)[1][sae.cfg.hook_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f4d5a1",
   "metadata": {},
   "source": [
    "# Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c740bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imagenet_train = load_dataset('Maysee/tiny-imagenet', split='train')\n",
    "imagenet_val_combined = load_dataset('Maysee/tiny-imagenet', split='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f60d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_val_test = imagenet_val_combined.train_test_split(test_size=0.5, stratify_by_column='label')\n",
    "imagenet_val = imagenet_val_test['train']\n",
    "imagenet_test = imagenet_val_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfc3b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageNetDataset(Dataset):\n",
    "    def __init__(self, huggingface_dataset, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            huggingface_dataset: Our ImageNet dataset from huggingface\n",
    "            transform: Potential transformation for the images\n",
    "        \"\"\"\n",
    "        self.dataset = huggingface_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx]['image']\n",
    "        label = self.dataset[idx]['label']\n",
    "\n",
    "        # Apply the transform if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13e79182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm \n",
    "import h5py\n",
    "import gzip\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Example transformation function: this is for use with Vision Transformers\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x), # apparently some images are not RGB\n",
    "])\n",
    "\n",
    "train_dataset = ImageNetDataset(imagenet_train, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e158109e",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98df9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_top_k(c):\n",
    "    # Create a list of all the images of the class\n",
    "    class_images = imagenet_train[500 * c: 500 * (c + 1)]['image']\n",
    "            \n",
    "    neuron_activations = []\n",
    "    for image in tqdm(class_images):\n",
    "        encoded = sae.encode(run_model(prompt, image)[1][sae.cfg.hook_name]).cpu()\n",
    "        neuron_activations.append(encoded)\n",
    "\n",
    "    stacked = torch.stack(neuron_activations)\n",
    "    return torch.topk(stacked, 10, dim=-1).indices   \n",
    "\n",
    "def gather_activations(c):\n",
    "    # Create a list of all the images of the class\n",
    "    class_images = imagenet_train[500 * c: 500 * (c + 1)]['image']\n",
    "            \n",
    "    neuron_activations = []\n",
    "    for image in tqdm(class_images):\n",
    "        encoded = sae.encode(run_model(prompt, image)[1][sae.cfg.hook_name]).cpu()\n",
    "        neuron_activations.append(encoded)\n",
    "\n",
    "    stacked = torch.stack(neuron_activations)\n",
    "    return stacked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "for c in tqdm(range(200)):\n",
    "    activations.append(gather_activations(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b0f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain class mapping\n",
    "import json\n",
    "\n",
    "with open(\"gemma-2b_12-res-jb.json\") as descs:\n",
    "    descs = json.load(descs)\n",
    "\n",
    "descs = {int(i[\"index\"]): i[\"description\"] for i in descs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09fea75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts(cls, n=500, idx=586):\n",
    "    acts = torch.zeros(sae.cfg.d_sae, device=model.device)\n",
    "    for i in tqdm(imagenet_train[cls*500:cls*500+n][\"image\"]):\n",
    "        acts += sae.encode(run_model(prompt, i, stop_at_layer=sae.cfg.hook_layer + 1)[1][sae.cfg.hook_name])[0, idx]\n",
    "    return acts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = []\n",
    "# Gather activations for 200 classes of ImageNet\n",
    "for i in range(200):\n",
    "    acts.append(get_acts(i, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "688e51fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = torch.stack(acts)\n",
    "torch.save(stacked, \"activations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e55f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"classes.csv\"\n",
    "df = pd.read_csv(file_path, sep=\" \", names=[\"index\", \"class\"])\n",
    "df.set_index(\"index\", inplace=True)\n",
    "df = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6976a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top(acts, k=10, blacklist=(16088, 13449, 15314,  2777, 9392, 9987, 1645, 10989, 13140, 2722)):\n",
    "    for i in acts.topk(k).indices:\n",
    "        i = i.item()\n",
    "        if i not in blacklist and (d := descs.get(i)):\n",
    "            print(i, d)\n",
    "\n",
    "for idx, act in enumerate(acts):\n",
    "    print(\"CLASS\", idx, df[idx])\n",
    "    show_top(act)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd22ae4",
   "metadata": {},
   "source": [
    "## Test text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58135f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image = imagenet_train[187*500+1]['image']\n",
    "# show image\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ca4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [{\"role\": \"user\", \"content\": processor.image_token + \"\\nDescribe the image\"}]\n",
    "prompt = processor.tokenizer.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "def generate(prompt, image, **kwds):\n",
    "    inp = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "    streamer = transformers.TextStreamer(processor.tokenizer, True, skip_special_tokens=True)\n",
    "    model.generate(**inp, max_new_tokens=256, streamer=streamer, eos_token_id=107, **kwds)\n",
    "\n",
    "generate(prompt, image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b676a",
   "metadata": {},
   "source": [
    "## Intervene on the activation functions of the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79337add",
   "metadata": {},
   "source": [
    "### Let's try different scales of intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27 - cat\n",
    "# 4501 - feature in SAE encoding space that corresponds to friendly animals (particuarly dogs)\n",
    "# Subtract this activation vector in the residual stream of the 12th block of the transformer\n",
    "hooked_generate(prompt, imagenet_train[187*500+1][\"image\"], fwd_hooks=make_hook(4501, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27 - cat\n",
    "# 4501 - feature in SAE encoding space that corresponds to friendly animals (particuarly dogs)\n",
    "# Subtract this activation vector in the residual stream of the 12th block of the transformer\n",
    "hooked_generate(prompt, imagenet_train[187*500+1][\"image\"], fwd_hooks=make_hook(4501, -3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d28102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_generate(prompt, imagenet_train[187*500+1][\"image\"], fwd_hooks=make_hook(4501, -4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266ff7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A really large scale intervention produces gibberish output, which makes sense\n",
    "# hooked_generate(prompt, imagenet_train[187*500+1][\"image\"], fwd_hooks=make_hook(4501, -20))\n",
    "\n",
    "hooked_generate(prompt, imagenet_train[187*500+1][\"image\"], fwd_hooks=make_hook(4501, -6))\n",
    "\n",
    "# becoming gibberish\n",
    "# anything beyond this becomes completely incoherent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A really large scale intervention produces gibberish output, which makes sense\n",
    "# hooked_generate(prompt, imagenet_train[187*500+1][\"image\"], fwd_hooks=make_hook(4501, -20))\n",
    "\n",
    "hooked_generate(prompt, imagenet_train[187*500+1][\"image\"], fwd_hooks=make_hook(4501, -7))\n",
    "\n",
    "# becoming gibberish\n",
    "# anything beyond this becomes completely incoherent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd8bd51",
   "metadata": {},
   "source": [
    "### What happens when you intervene with a gibberish activation?\n",
    "\n",
    "it seems that the noise is handled by the model, which is unexpected...\n",
    "the noise is normally distributed, so maybe that's why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7e41fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibberish_hook(magnitude):\n",
    "    # encoded = torch.zeros(sae.cfg.d_sae, device=model.device)\n",
    "    encoded = torch.randn(sae.cfg.d_sae, device=model.device) * magnitude\n",
    "    bias = sae.decode(encoded)\n",
    "    decoded = sae.decode(encoded) - bias\n",
    "    def boost(value, hook):\n",
    "        value += decoded\n",
    "        return value\n",
    "    return [(sae.cfg.hook_name, boost)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc618650",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_generate(prompt, imagenet_train[187*500+1][\"image\"], fwd_hooks=gibberish_hook(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bf00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_generate(prompt, imagenet_train[187*500+1][\"image\"], fwd_hooks=gibberish_hook(1000000000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1f4de289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hook(idx, amt):\n",
    "    encoded = torch.zeros(sae.cfg.d_sae, device=model.device)\n",
    "    bias = sae.decode(encoded)\n",
    "    encoded[idx] = amt\n",
    "    decoded = sae.decode(encoded) - bias\n",
    "    def boost(value, hook):\n",
    "        value += decoded\n",
    "        return value\n",
    "    return [(sae.cfg.hook_name, boost)]\n",
    "\n",
    "def crafted_gibberish_hook(magnitude):\n",
    "    encoded = torch.zeros(sae.cfg.d_sae, device=model.device)\n",
    "    bias = sae.decode(encoded)  \n",
    "    encoded[4501] = magnitude  \n",
    "    encoded[4502] = magnitude\n",
    "    encoded[4503] = magnitude\n",
    "    encoded[4504] = magnitude\n",
    "    encoded[4505] = magnitude\n",
    "    encoded[4506] = magnitude\n",
    "    encoded[4507] = magnitude\n",
    "    encoded[4508] = magnitude\n",
    "    decoded = sae.decode(encoded) - bias\n",
    "    def boost(value, hook):\n",
    "        value += decoded\n",
    "        return value\n",
    "    return [(sae.cfg.hook_name, boost)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d895cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_generate(prompt, imagenet_train[187*500+1][\"image\"], fwd_hooks=crafted_gibberish_hook(-2))\n",
    "\n",
    "# gibberish intervention indeed produces gibberish output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8dad9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_noise_hook(noise_scale=0.1):\n",
    "    \"\"\"\n",
    "    Creates a hook that injects random Gaussian noise into the activations.\n",
    "    \n",
    "    Args:\n",
    "        noise_scale: Standard deviation of the noise to add\n",
    "        \n",
    "    Returns:\n",
    "        A hook that can be used with hooked_model.run_with_hooks()\n",
    "    \"\"\"\n",
    "    def boost(value, hook):\n",
    "        # Generate random noise with the same shape as the value tensor\n",
    "        noise = torch.randn_like(value) * noise_scale\n",
    "        # Add the noise to the value\n",
    "        value += noise\n",
    "        return value\n",
    "    \n",
    "    return [(sae.cfg.hook_name, boost)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561dd77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_generate(prompt, imagenet_train[187*500+1][\"image\"], fwd_hooks=random_noise_hook(1.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9fa388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inject cat into dog\n",
    "# 27 - cat\n",
    "# 4501 - feature in SAE encoding space that corresponds to friendly animals (particuarly dogs)\n",
    "# Subtract this activation vector in the residual stream of the 12th block of the transformer\n",
    "hooked_generate(prompt, imagenet_train[187*500+1][\"image\"], fwd_hooks=make_hook(27, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2070f8a",
   "metadata": {},
   "source": [
    "### Let's try a different image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_image = Image.open(\"cat_image.png\")\n",
    "display(cat_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [{\"role\": \"user\", \"content\": processor.image_token + \"\\nDescribe the image\"}]\n",
    "prompt = processor.tokenizer.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "def generate(prompt, image, **kwds):\n",
    "    inp = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "    streamer = transformers.TextStreamer(processor.tokenizer, True, skip_special_tokens=True)\n",
    "    model.generate(**inp, max_new_tokens=256, streamer=streamer, eos_token_id=107, **kwds)\n",
    "\n",
    "generate(prompt, cat_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e2bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = torch.zeros(sae.cfg.d_sae, device=model.device)\n",
    "acts += sae.encode(run_model(prompt, cat_image, stop_at_layer=sae.cfg.hook_layer + 1)[1][sae.cfg.hook_name])[0, 586]\n",
    "\n",
    "show_top(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_generate(prompt, cat_image, fwd_hooks=make_hook(4501, -3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d60ea",
   "metadata": {},
   "source": [
    "it's interesting that the model removed the cat from the description but still recognizes the green background"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
